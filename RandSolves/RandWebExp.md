# Inspect HTML [E]
easy just inspect its there in comment format.

# Scavenger Hunt [E]
So the first part is in html only about the second part here is the extract from an article 

The second part we get from the css and in the js file there is hint which is basically this off an article : 

> how Google can keep track of all these websites? It uses the Google web crawling/spider engine. 
But since devs don't want these spiders to reach and index every part of the website, we use a special file called 
robots.txt! This file tells the crawlers what parts of the site are disallowed, and what User Agents are allower to visit
Crawlers that listen to these files are called polite, when a crawler is not polite it could fall into something called a 
spider trap.

to access robots.txt we just put in after the domain, here we get the part 3 
![image](https://github.com/user-attachments/assets/44257ae2-e2a3-434b-a6ca-1a6cdd14f9b7)

now part 4 is apache server so 
>A .htaccess file is a configuration file that controls and configures Apache web servers. It's short for "Hypertext Access". You can use a .htaccess file to: 
Configure a server's initial settings 
Make the server behave in a certain way 
Protect your site with a password 
Create a custom error page 

now for the part 5 we have to access something called 
>.DS_Store is a special MacOS file that stores information about the current folder

# Local Authority [E]
We see how the password works and in secure.js file there is username and password which gives the flag.


